"""
AI Vulnerability Classifier
Classify and evaluate AI-specific security vulnerabilities
"""
from typing import Dict, List, Optional


class AIVulnerabilityClassifier:
    """AI security vulnerability classification"""
    
    VULNERABILITY_TYPES = {
        'prompt_injection': {
            'system_prompt_leakage': {
                'severity': 'high',
                'description': 'System prompt leakage',
                'impact': 'Attackers can understand AI limitations and instructions to design more precise attacks'
            },
            'jailbreak': {
                'severity': 'high',
                'description': 'Jailbreak attack',
                'impact': 'Bypass AI security restrictions, generate harmful content'
            },
            'instruction_override': {
                'severity': 'high',
                'description': 'Instruction override',
                'impact': 'Override system instructions, execute malicious operations'
            },
            'context_poisoning': {
                'severity': 'medium',
                'description': 'Context poisoning',
                'impact': 'Inject malicious instructions through context'
            },
            'indirect_injection': {
                'severity': 'medium',
                'description': 'Indirect injection',
                'impact': 'Inject malicious instructions indirectly through translation, rewriting, etc.'
            }
        },
        'data_exfiltration': {
            'training_data_extraction': {
                'severity': 'high',
                'description': 'Training data extraction',
                'impact': 'Extract AI model training data, may leak sensitive information'
            },
            'user_data_leakage': {
                'severity': 'high',
                'description': 'User data leakage',
                'impact': 'Leak other users\' conversations or data'
            },
            'model_information_disclosure': {
                'severity': 'medium',
                'description': 'Model information disclosure',
                'impact': 'Leak sensitive information such as model architecture and parameters'
            }
        },
        'authorization_bypass': {
            'idor_via_prompt': {
                'severity': 'high',
                'description': 'Implement IDOR via Prompt',
                'impact': 'Access unauthorized data through carefully designed prompts'
            },
            'privilege_escalation': {
                'severity': 'high',
                'description': 'Privilege escalation',
                'impact': 'Bypass permission control through AI'
            },
            'access_control_bypass': {
                'severity': 'high',
                'description': 'Access control bypass',
                'impact': 'Bypass access control mechanisms'
            }
        },
        'adversarial_attacks': {
            'adversarial_prompts': {
                'severity': 'medium',
                'description': 'Adversarial prompts',
                'impact': 'Mislead AI using adversarial inputs'
            },
            'model_poisoning': {
                'severity': 'high',
                'description': 'Model poisoning',
                'impact': 'Poison model through malicious inputs'
            },
            'backdoor_activation': {
                'severity': 'high',
                'description': 'Backdoor activation',
                'impact': 'Activate backdoors in the model'
            }
        }
    }
    
    def __init__(self):
        self.severity_weights = {
            'high': 3,
            'medium': 2,
            'low': 1
        }
    
    def classify_vulnerability(self, test_result: Dict) -> Dict:
        """
        Classify vulnerability
        
        Parameters:
            test_result: Test result
            
        Returns:
            Classified vulnerability information
        """
        category = test_result.get('category', 'unknown')
        severity = test_result.get('severity', 'unknown')
        
        # Find corresponding vulnerability type
        vuln_type = None
        vuln_info = None
        
        for vuln_category, vuln_types in self.VULNERABILITY_TYPES.items():
            if category in vuln_types:
                vuln_type = vuln_category
                vuln_info = vuln_types[category]
                break
        
        if not vuln_info:
            # Default classification
            vuln_type = 'prompt_injection'
            vuln_info = {
                'severity': severity,
                'description': f'{category} vulnerability',
                'impact': 'Potential security risk'
            }
        
        return {
            'type': vuln_type,
            'subtype': category,
            'severity': vuln_info.get('severity', severity),
            'description': vuln_info.get('description', ''),
            'impact': vuln_info.get('impact', ''),
            'evidence': test_result.get('evidence', ''),
            'payload': test_result.get('payload', ''),
            'url': test_result.get('url', '')
        }
    
    def calculate_risk_score(self, vulnerabilities: List[Dict]) -> float:
        """
        Calculate risk score
        
        Parameters:
            vulnerabilities: Vulnerability list
            
        Returns:
            Risk score (0-100)
        """
        if not vulnerabilities:
            return 0.0
        
        total_score = 0
        for vuln in vulnerabilities:
            severity = vuln.get('severity', 'low')
            weight = self.severity_weights.get(severity, 1)
            total_score += weight * 10
        
        # Normalize to 0-100
        max_score = len(vulnerabilities) * 30  # Assume all are high
        risk_score = (total_score / max_score) * 100 if max_score > 0 else 0
        
        return min(100, risk_score)
    
    def get_recommendations(self, vulnerabilities: List[Dict]) -> List[str]:
        """Get remediation recommendations"""
        recommendations = []
        
        # Group by type
        by_type = {}
        for vuln in vulnerabilities:
            vuln_type = vuln.get('type', 'unknown')
            if vuln_type not in by_type:
                by_type[vuln_type] = []
            by_type[vuln_type].append(vuln)
        
        # Generate recommendations
        if 'prompt_injection' in by_type:
            recommendations.append(
                "Implement input validation and filtering, detect and block prompt injection attempts"
            )
            recommendations.append(
                "Use output filtering to ensure AI responses do not contain sensitive information"
            )
        
        if 'data_exfiltration' in by_type:
            recommendations.append(
                "Implement data access control to limit AI access to sensitive data"
            )
            recommendations.append(
                "Monitor AI responses to detect abnormal data leakage"
            )
        
        if 'authorization_bypass' in by_type:
            recommendations.append(
                "Implement permission checks at the application layer, do not rely on AI for authorization"
            )
            recommendations.append(
                "Use independent authentication and authorization mechanisms"
            )
        
        return recommendations

